The task at hand is to develop a big data pipeline to process and analyze a log file in real time. The pipeline starts with feeding the data into a Kafka producer, which will publish the data to a Kafka topic. A Kafka consumer, subscribed to the same topic, periodically checks for new data. Once the data is available, Spark is used to perform transformations and conduct Exploratory Data Analysis (EDA) on the received data.
