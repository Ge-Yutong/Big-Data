Using python to select top n words in large text file(16GB), filtered stopwords.
Optimized method: 
1. Parallel Computing: The code employs the Python multiprocessing library to create a pool of worker processes, each process executing on a different CPU core. This allows the code to process different chunks of the text file in parallel, greatly reducing the overall time complexity. The function Pool(cpu_count()) creates a pool of worker processes equal to the number of CPU cores available, which allows for maximum CPU utilization. Each worker process executes the count_words function on a different chunk of the file.
2. Memory Mapping: To avoid loading the entire file into memory at once, the mmap module is used, which creates a memory map of the file. This allows for random access of file contents while only loading the necessary portions into memory, reducing memory usage.
3. Chunk-based Reading: Instead of reading the whole file at once, it reads the file in chunks. This is especially useful when dealing with large files that cannot fit into memory.
4. Handling Stop Words: The script efficiently handles stop words by pre-loading them into a list and then filtering them out during the word counting process.
5. Efficient Text Processing: The code uses the Python collections.Counter class for efficiently counting the frequency of each word in the text. This class implements a dictionary-like object where the keys are the unique elements in the input data, and the values are the counts of these elements. The Counter class provides a most_common(n) method, which returns the n most common elements and their counts.
